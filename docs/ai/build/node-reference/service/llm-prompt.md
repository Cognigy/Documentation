---
title: "LLM Prompt"
slug: "llm-prompt"
description: "The Cognigy LLM Prompt Node allows using Generative AI for creating relevant content. This Node combines the capabilities of the AI Agent Node and the legacy LLM Prompt Node by adding support for Tools, including MCP tools."
hidden: false
tags:
  - new llm prompt node
  - llm prompt node
---

# LLM Prompt

[![Version badge](https://img.shields.io/badge/Added in-v2025.20-blue.svg)](../../../../release-notes/2025.20.md)

<figure>
  <img class="image-center" src="../../../../../_assets/ai/build/node-reference/services/llm-prompt.png" width="50%" />
</figure>

## Description

The LLM Prompt Node lets you configure the system prompt for your LLM calls, generating both text and structured content. With this Node, you can also:

- Include tools in your LLM calls.
- Enable image handling.
- Set advanced LLM request options.

Additionally, you can refer to your AI Agent configuration using the [Load AI Agent Node](../ai/load-ai-agent.md).

Before using this Node, set the LLM provider in the [Settings](../../../empower/generative-ai.md).

## LLM Prompt Settings

??? info "Large Language Model"
    The selected **Default** model is the model that you specified in **Settings > Generative AI Settings** of your Project.
    
    You can select a different model from the list or override the selected model using the Custom Model Options parameter.

??? info "System Prompt"
    The system prompt is the message sent to the LLM to guide its responses. The parameter supports CognigyScript, allowing dynamic content and logic within the prompt. This prompt can work either as the input for completion tasks or as the system message in chat-based interactions, setting context and behavior for the AI Agent.
    
    Additionally, you can inject the recent conversation into the **System Prompt** field by using these tags:
    
    - `@cognigyRecentConversation` — the tag is replaced with a string that can contain up to 10 recent AI Agent and 10 user outputs, for example:
       ```text
       Bot: agentOutput1
       User: userOutput1
       Bot: agentOutput2
       User: userOutput2
       ```
    - `@cognigyRecentUserInputs` — the tag is replaced with a string that can contain up to 10 recent user outputs, for example:

       ```text
       User: userOutput1
       User: userOutput2
       ```

    If you want to access only the last user input, specify the `Text` token in the **System Prompt** field.

    
    When adding a tag, ensure that you leave a line break before and after the tag, for example:
    
    ```text
    A user had a conversation with a chatbot. The conversation history so far is:
    @cognigyRecentConversation
    
    Describe the user sentiment in one very short line.
    ```
    
    Both tags can include an optional turn limit parameter, which is appended to the tag.
    
    Examples:
    
    ```typescript
    @cognigyRecentConversation:3 // returns the last 3 turns of the conversation.
    @cognigyRecentUserInputs:2 // returns the last 2 user inputs.
    ```

??? info "Advanced"

    | Parameter                  | Type   | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
    |----------------------------|--------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
    | Maximal Tokens             | Slider | The maximum number of tokens to generate in the completion.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
    | Use Single Prompt Mode     | Toggle | Send a single prompt to the model without any conversation context. This parameter is disabled by default. It doesn't support multi-turn conversations or chat and is useful for simple, one-off completions.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
    | Transcript Turns           | Slider | The number of conversation turns to include in the LLM chat completion request. By default, the value is `50`.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
    | Response Format            | Select | Choose the format for the model's output result. You can select one of the following options:<ul><li>**None** — no response format is specified, or do not request with an LLM provider that does not accept any response format or does not support it or could use provider's default in-built response format. This option is selected by default.</li><li>**Text** — the model returns messages in text format.</li><li>**JSON Object** — the model returns messages in JSON format. To use this option, instruct the model to generate JSON via a system or user message in the **Instruction (System Message/Prompt)** field. For example, `Take the user message and return it as JSON in the following format {"message": "THE_MESSAGE"}`. Note that this parameter may not be supported by all LLMs. For more information, refer to the LLM provider's API documentation.</li></ul>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
    | Timeout                    | Number | The maximum number of milliseconds to wait for a response from the Generative AI Provider.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
    | Sampling Method            | Select | Methods:<ul><li>**Temperature** — determines the level of randomness in the generated text. A higher temperature allows for more diverse and creative outputs, while a lower temperature leads to more predictable and consistent outputs with the training data.</li><li>**Top Percentage** — specifies the percentage of the most probable outputs for generation, resulting in more consistent output.</li></ul>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
    | Temperature                | Slider | Define the sampling temperature, which ranges between 0 and 1. Higher values, such as 0.8, make the output more random, while lower values, such as 0.2, make it more focused and deterministic.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
    | Top Percentage             | Slider | Control the Top-p (nucleus) sampling, ranging from 0 to 1. Higher values allow more diverse word choices, while lower values make the output more focused. For example, 0.9 means the model selects from the smallest set of words with a combined probability of 90%.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
    | Presence Penalty           | Slider | Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood of talking about new topics.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
    | Frequency Penalty          | Slider | Number between -2.0 and 2.0. The penalty assigns a lower probability to tokens frequently appearing in the generated text, encouraging the model to generate more diverse and unique content.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
    | Use Stops                  | Toggle | Whether to use a list of stop words to let Generative AI know where the sentence stops.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
    | Stops                      | Text   | Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
    | Seed                       | Number | Use this parameter for consistent output when referring to the same LLM Prompt Node multiple times. Specify any integer number, for example, `123`. The number in the Seed field and the prompt in the **Instruction (System Message/Prompt)** field should remain unchanged during subsequent references to the Node.<br>Note that in OpenAI, this parameter is in [Beta](https://platform.openai.com/docs/api-reference/chat/create#chat-create-seed) and is supported only by [certain models](https://cookbook.openai.com/examples/reproducible_outputs_with_the_seed_parameter).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
    | Include Rich Media Context | Toggle | Controls whether _context_ is added to the prompt. In this case, _context_ refers to text extracted from rich media such as Text with Buttons, Quick Replies, and [other types](../basic/say.md#output-type). This text provides AI Agents with additional information, improving their responses.<br><br>If the [Textual Description](../basic/say.md#output-type) parameter in the Say, Question, or Optional Question Node is filled, the context is taken only from this parameter. If the **Textual Description** parameter is empty, the context is taken from the button titles and alt text in the rich media. By default, the **Include Rich Media Context** parameter is active. When this parameter is inactive, no context is added.<br><br>**Examples**:<ul><li>If **Textual Description** is filled:<p>Textual Description: `Select your preferred delivery option: Standard Delivery or Express Delivery`.</p><p>Quick Replies' buttons: `Standard Delivery`, `Express Delivery`.</p><p>Context added to the prompt: `Select your preferred delivery option: Standard Delivery or Express Delivery`.</p></li><li>If **Textual Description** is empty:<p>Textual Description: empty.</p><p>Quick Replies' buttons: `Standard Delivery`, `Express Delivery`.</p><p>Context added to the prompt: `Standard Delivery`, `Express Delivery`.</p></li><li>If **Include Rich Media Context** is inactive:<p>No context is added to the prompt.</p></li></ul> |

??? info "Storage & Streaming Options"

    | Parameter                     | Type          | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
    |-------------------------------|---------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
    | How to handle the result      | Select        | Determine how to handle the prompt result:<ul><li>**Store in Input** — stores the result in the Input object. To print the prompt result, use the LLM Prompt Result Token in the Say Node or enable the **Output result immediately** option.</li><li>**Store in Context** — stores the result in the Context object. To print the prompt result, use the LLM Prompt Result Token in the Say Node or enable the **Output result immediately** option.</li><li>**Stream to Output** — streams the result directly into the output. This means that the model provides prompts directly into the conversation chat, as soon as a Stream Buffer Flush Token is matched, and you don't need to use the LLM Prompt Result token and Say Node. By default, this result won't be stored in either the Input or the Context. You can change this behavior by activating the **Store Copy in Input** option. Note that streaming may not be supported by all [Cognigy LLM Prompt Node](../../../empower/llms/model-support-by-feature.md) providers, such as Google[^*]. If streaming is not supported, enabling the **Store Copy in Input** option will save the result to the Input object.</li></ul> |
    | Input Key to store Result     | CognigyScript | The parameter appears when **Store in Input** is selected. The result is stored in the `promptResult` Input object by default. You can specify another key.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
    | Context Key to store Result   | CognigyScript | The parameter appears when **Store in Context** is selected. The result is stored in the `promptResult` Context object by default. You can specify another key.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
    | Stream Buffer Flush Tokens    | Text Array    | The parameter appears when **Stream to Output** is selected. It defines tokens that trigger the stream buffer to flush to the output. The tokens can be punctuation marks or symbols, such as `\n`.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
    | Stream Buffer Flush Overrides | Text Array    | The parameter appears when **Stream to Output** is selected. It allows using regular expressions (without leading or trailing slashes) to control stream buffer flushing. A trailing `$` is automatically added to match patterns at the end of the buffer. For example, `\d+\.` checks for a number followed by a dot at the end of the string.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
    | Output result immediately     | Toggle        | The parameter appears when you select either **Store in Input** or **Store in Context**. This parameter allows you to output results immediately without using the Say Node and LLM Prompt token.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
    | Store Detailed Results        | Toggle        | The parameter appears when you select either **Store in Input** or **Store in Context**, or when you enable **Store Copy in Input**. This parameter allows you to save detailed results of the LLM's generated output. By default, the result is stored in the `promptResult` object. You can specify another value in the **Context Key for the Result** field to save it in the Context object, or in the **Input Key for the Result** to save it in the Input object. The object contains keys such as `result`, `finishReason`, and `usage`. It may also include `detailedResult` if completion models are used, as well as `firstChunk` and `lastChunk` in some streaming results, depending on the LLM provider.                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
    | Store Copy in Input           | Toggle        | The parameter appears when **Stream to Output** is selected. In addition to streaming the result to the output, store a copy in the Input object by specifying a value in the **Input Key to store Result** field.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |

    **Streaming Results**

    In Stream mode, LLM generates tokens and returns them one by one to Cognigy.AI to ensure low-latency responses.
    Cognigy.AI monitors delimiter tokens (Stream Buffer Flush Tokens),
    which serve as markers indicating when to output the token buffer.
    These tokens could be `.`, `!`, `?`, or any other symbols that act as delimiters for complete logical statements.
    When Cognigy.AI detects one of these tokens, it promptly flushes the token buffer into the voice or text chat.

    The preconfigured overrides are listed in the table.

    | Regex                                 | Description                                | Example    |
    |---------------------------------------|--------------------------------------------|------------|
    | `\d+\.`                               | A number followed by a dot.                | 26.08      |
    | `\b(?:Dr|Ms|Mr|Mrs|Prof|Sr|Jr|ca)\.`  | Common abbreviations followed by a dot.    | Mr.        |
    | `\b[A-Z]\.`                           | A single capital letter followed by a dot. | M. Smith   |
    | `\.\.\.`                              | Three dots used for omission.              | ...        |
    | `\b.\..\.`                            | Two-letter abbreviations.                  | i.e., e.g. |

    [^*]: Note that not all LLM models support streaming.

??? info "Tool Settings"
    The process of setting up a tool is the same as for the AI Agent Node. See the example in the [AI Agent Tool Settings](../ai/ai-agent.md#ai-agent-tool-settings) section.

    | Parameter       | Type     | Description                                                                                                                                                                                                                                                                                                                         |
    |-----------------|----------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
    | Tool Choice     | Selector | If supported by your LLM Model, this will determine how tools should be selected by the AI Agent:<ul><li>**Auto** — tools (or none) are automatically selected by the AI Agent when needed.</li><li>**Required** — your AI Agent will always use one of its Tools.</li><li>**None** — your AI Agent won't use a tool.</li></ul>     |
    | Use Strict mode | Toggle   | When the parameter is enabled, strict mode (if supported by the LLM provider) ensures that the arguments passed to a tool call precisely match the expected parameters. Enabling this feature can help prevent errors. However, it may cause a slight delay in the response, especially during the first call after making changes. |

??? info "Image Handling"
    | Parameter            | Type     | Description                                                                                                                                                                                                                                                                                                                                |
    |----------------------|----------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
    | Process Images       | Toggle   | Enable the AI Agent to read and understand images attachments. Make sure that your LLM provider supports image processing; refer to your provider's documentation. In addition, make sure that attachments are supported by and activated in your Endpoint, for example, Webchat.                                                          |
    | Images in Transcript | Selector | Configure how images older than the last turn are handled to reduce token usage: <ul><li>**Minify** — reduces the size of these images to 512x512px.</li><li>**Drop** — excludes the images.</li><li>**Keep** — sends the max size (this option consumes more tokens).</li></ul> Limitations and token consumption depend on the LLM used. |

??? info "Error Handling"

    | Parameter                      | Type   | Description                                                                                                                                                                                                                                                                                                                                                                                                              |
    |--------------------------------|--------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
    | Log to System Logs             | Toggle | Log errors to the system logs. They can be viewed on the [Logs](../../../test/logs.md) page of your Project. The parameter is inactive by default.                                                                                                                                                                                                                                                                       |
    | Select Error Handling Approach | Select | You can select one of the Error Handling options:<ul><li>**Stop Flow Execution** — terminate the current Flow execution.</li><li>**Continue Flow Execution** — allow the Flow to continue executing, bypassing the error and proceeding to the next steps.</li><li>**Go to Node** — redirect the workflow to a specific Node in the Flow, which can be useful for error recovery or customized error handling.</li></ul> |
    | Error Message (optional)       | Text   | The parameter appears when **Continue Flow Execution** is selected. Add an message to output if the LLM Prompt Node fails.                                                                                                                                                                                                                                                                                               |
    | Select Flow                    | Select | The parameter appears when **Go to Node** is selected. Select a Flow from the available options.                                                                                                                                                                                                                                                                                                                         |
    | Select Node                    | Select | The parameter appears when **Go to Node** is selected. Select a Node from the available options.                                                                                                                                                                                                                                                                                                                         |

??? info "Custom Options"

    These settings are helpful if you need to use parameters that are not included in the LLM Prompt Node or if you need to overwrite existing ones.

    | Parameter              | Type | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
    |------------------------|------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
    | Custom Model Options   | JSON | Additional parameters for the LLM model. You can specify individual parameters as well as entire functions. These parameters customize the behavior of the model, such as adjusting temperature, top_k, or presence_penalty. Note that if you use a parameter already set in the Node, for example, temperature, it will be overwritten. To view the full list of available parameters for your model, refer to the LLM provider's API documentation. For example, [OpenAI](https://platform.openai.com/docs/api-reference/chat/create) or [Azure OpenAI](https://learn.microsoft.com/en-us/azure/ai-services/openai/).<br><br> **Examples:** <ul><li>`{ "logprobs": true }`</li><li>`{ "temperature": 0.7 }`</li><li>`{ "model": "claude-3-5-sonnet-20240620" }` (see Forcing Model Versions)</li></ul> |
    | Custom Request Options | JSON | Additional parameters for the LLM request. These parameters customize the request itself, such as setting parameters related to timeout, retries, or headers. For more information, refer to the LLM provider's API documentation. <br><br> **Examples:** <br>- `{ "timeout": 5000 }`<br> - `{ "headers": { "Authorization": "Bearer <token>" } }`                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
  
    **Forcing Model Versions**

    You can force the LLM Prompt Node to use a specific model version by including it in the Custom Model Options.
    This means that the LLM Prompt Node will use the specified version of the language model instead of the default or any other available versions. This allows for more control over the behavior of the LLM Prompt Node, ensuring it utilizes the desired model version for generating prompts or responses.

    You can use models from any LLM provider supported by Cognigy, including those not yet directly integrated.
    However, you can only replace one model with another within the same provider.

    Let's consider an example with the Anthropic provider:
    how to force the LLM Prompt Node to use the model version `claude-3-sonnet-20240229`,
    despite the LLM resource defaulting to the `claude-3-5-sonnet-20240620` model:

    1. [Create an Anthropic LLM resource](../../../empower/llms/providers/anthropic.md) for Claude-1, for example, `claude-3-5-sonnet-20240620`.
    2. Create a Flow and add an LLM Prompt Node to it.
    3. In the LLM Prompt Node, select the model `claude-3-5-sonnet-20240620` from the **Large Language Model** list.
    4. Override the model selection. In the **Custom Model Options** field, specify the custom model options as follows: `{ "model": "claude-3-sonnet-20240229" }`.
    5. Click **Save Node**.

    The LLM Prompt Node now utilizes the `claude-3-sonnet-20240229` model.

    Below, you'll find documentation for supported models:

    - [Aleph Alpha Models](https://docs.aleph-alpha.com/)
    - [Anthropic Models](https://docs.anthropic.com/claude/docs/models-overview)
    - [Azure OpenAI](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models)
    - [OpenAI Models](https://platform.openai.com/docs/models)

??? info "Debugging Settings"

    When using the Interaction Panel, you can trigger two types of debug logs. These logs are only available when using the Interaction Panel and are not intended for production debugging. You can also combine both log types.

    | Parameter                      | Type          | Description                                                                                                                                                                                                                                                                                                                                                                                             |
    |--------------------------------|---------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
    | Show Token Count               | Toggle | Send a debug message containing the input, output, and total token count. The message appears in the Interaction Panel when [debug mode](../../../test/interaction-panel/chat.md#debug-mode) is enabled. Cognigy.AI uses the GPT-3 tokenizer algorithm, so actual token usage may vary depending on the model used. The parameter is inactive by default. |
    | Log System Prompt & Completion | Toggle | Send a debug message containing the system prompt sent to the LLM provider and the subsequent completion. The message appears in the Interaction Panel when [debug mode](../../../test/interaction-panel/chat.md#debug-mode) is enabled. The parameter is inactive by default.                                                                            |
    | Log Tool Definitions           | Toggle | Send a debug message containing information about the configured AI Agent tools. The message appears in the Interaction Panel when [debug mode](../../../test/interaction-panel/chat.md#debug-mode) is enabled. The parameter is inactive by default.                                                                                                     |
    | Log LLM Latency                | Toggle | Send a debug message containing key latency metrics for the request to the model, including the time taken for the first output and the total time to complete the request. The message appears in the Interaction Panel when [debug mode](../../../test/interaction-panel/chat.md#debug-mode) is enabled. The parameter is inactive by default.          |
    | Send request logs to Webhook   | Toggle        | Send the request sent to the LLM provider and the subsequent completion to a webhook service, including metadata, the request body, and custom logging data. With this parameter, you can use a webhook service to view detailed logs of the request to the LLM. The parameter is inactive by default.                                                                                                  |
    | Webhook URL                    | CognigyScript | Enter the URL of the webhook service to send the request logs to.                                                                                                                                                                                                                                                                                                                                       |
    | Custom Logging Data            | CognigyScript | Enter custom data to send with the request to the webhook service.                                                                                                                                                                                                                                                                                                                                      |
    | Condition for Webhook Logging  | CognigyScript | Enter the condition for the webhook logging.                                                                                                                                                                                                                                                                                                                                                            |
    | Webhook Headers                | Input fields  | Enter the headers to send with the request to the webhook service. Use the **Key** and **Value** fields to enter a header. The **Value** field supports CognigyScript. After entering the header key, new empty **Key** and **Value** fields are automatically added, in case you need to add more headers. Alternatively, you can click **Show JSON Editor** and add input examples in the code field. |

{! _includes/ai/build/node-reference/tools.md !}

## Examples

{! _includes/ai/build/node-reference/tool-examples.md !}

## More Information

- [LLM](../../../empower/llms/overview.md)
- [Generative AI](../../../empower/generative-ai.md)