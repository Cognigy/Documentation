---
title: "AI Agent"
slug: "ai-agent"
description: "This Node lets you assign an AI Agent to a job, provide guidance for that job, and configure access to the information the AI Agent can use to retrieve data."
hidden: false
---

# AI Agent

<figure>
  <img class="image-center" src="../../../../../_assets/ai/build/node-reference/ai/ai-agent.png" width="80%" />
</figure>

## Description

This Node lets you assign an AI Agent to a job, provide guidance and toolset for that job,
and configure access to the information the AI Agent can use to retrieve data.

To configure this Node, follow these steps:

1. [Define an AI Agent job](#ai-agent-settings).
2. [Define the toolset for performing this job](#ai-agent-tool-settings).

## AI Agent Settings

### AI Agent

This configuration assigns an AI Agent to a job, defines its role and responsibilities, and provides additional instructions or context to guide its actions.

| Parameter                | Type          | Description                                                                                                                                                                                                                                                                                            |
|--------------------------|---------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| AI Agent                 | Selector      | Select the AI Agent to assign to the job.                                                                                                                                                                                                                                                              |
| Job Name                 | CognigyScript | Specify the name of the job to define the AI Agent's role. For example, `Customer Support Specialist`.                                                                                                                                                                                                 |
| Job Description          | CognigyScript | Provide a description of the job responsibilities to guide the AI Agent's interactions. For example, `Assist customers with product issues, escalate complex cases, and provide guidance on best practices`.                                                                                           |
| Instructions and Context | Toggle        | Add specific instructions or context as a system message to help the AI Agent better fulfill the job requirements. For example, `Stay professional and friendly; focus on problem-solving and clarity`. These instructions will be considered in addition to those specified in the AI Agent settings. |

### Memory Handling

| Parameter                   | Type          | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
|-----------------------------|---------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Long-Term Memory Injection  | Selector      | Allow the AI Agent to access Contact Profile information for the current user. Select one of the following options:<br>None – No memory handling.<br>Inherit from AI Agent – uses the settings specified in the AI Agent Memory Configuration section.<br>Inject full Contact Profile – uses all information from the Contact Profile.<br>Inject Contact Memories only – uses information only from the Memories field within the Contact Profile.<br>Inject selected Profile fields – uses information from specific fields in the Contact Profile. |
| Selected Profile Fields     | Text          | Active when the **Use Job Knowledge** option is enabled. Enter specific fields from the Contact Profile for targeted data use. Specify the field using the [Profile keys](../../../analyze/contact-profiles.md#predefined-fields) format and press ++enter++ to apply it.                                                                                                                                                                                                                                                                            |
| Short-Term Memory Injection | CognigyScript | Specify the Context key that references a Context value to be injected into the AI Agent as short-term memory. This value will be retained throughout the session and cleared once the session ends.                                                                                                                                                                                                                                                                                                                                                 |

### Grounding Knowledge

| Parameter                  | Type          | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
|----------------------------|---------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Knowledge Injection        | Selector      | Use the Knowledge AI feature for the AI Agent. Select one of the following options:<br>  - **Never** — do not use the Knowledge Store.<br>- **When Required** — refer to the Knowledge Store when knowledge is required for a specific tool.  <br>- **Once for Each User Input** — refer to the Knowledge Store after each user input.                                                                                                                                                                                                                                                                                                                                                                                                                                                   | 
| Use AI Agent Knowledge     | Toggle        | Enable to use the Knowledge Store configured in the AI Agent                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
| Use Job Knowledge          | Toggle        | Enable to configure a Knowledge Store for this Job.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | 
| Job Knowledge Store        | Selector      | Active when the **Use Job Knowledge** option is enabled.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| Top K                      | Slider        | The Number of optimal matching chunks that the Knowledge AI solution should provide.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
| Source Tags                | CognigyScript | The tags serve to refine the scope of your knowledge search, allowing you to include only the most pertinent sections of the knowledge base and, as a result, improve the accuracy of search outputs. <br><br> Before specifying tags, ensure that they were provided during the creation of the Knowledge Sources. Add Tags by specifying each Tag separately and pressing ++enter++. The maximum number of tags is 5. <br><br>When you specify multiple Source Tags, the Search Extract Output Node defaults to an `AND` operator, meaning it only considers Sources that have all the specified Tags. This approach ensures the search results are precise and highly relevant to the end user's query. To change this behavior, go to the **Match Types for Source Tags** parameter. |
| Match type for Source Tags | Select        | The operator to filter Knowledge Sources by Source Tags. Select one of the following options:<br> - **AND** — the default value, requires all tags to match across multiple Knowledge Sources. Consider the following example: there are Knowledge Sources with Tags `S-a`, `S-b`, and `S-c`. When you use the `AND` operator to filter by `S-a` and `S-b`, only Sources with both Tags `S-a` and `S-b` will be included in the search results.<br> - **OR** — requires at least one tag to match across multiple Knowledge Sources. Consider the following example: there are Knowledge Sources with Tags `S-a`, `S-b`, and `S-c`. When you use the `OR` operator to filter by `S-a` or `S-b`, any Source with either Tag `S-a` or `S-b` will be included in the search results.        |

### Storage & Streaming Options

| Parameter                     | Type          | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
|-------------------------------|---------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| How to handle the result      | Select        | Determine how to handle the prompt result:<br> - **Store in Input** — stores the result in the Input object. To print the prompt result, use the LLM Prompt Result Token in the Say Node or enable the **Output result immediately** option. <br> - **Store in Context** — stores the result in the Context object. To print the prompt result, use the LLM Prompt Result Token in the Say Node or enable the **Output result immediately** option. <br>- **Stream to Output** — streams the result directly into the output. This means that the model provides prompts directly into the conversation chat, and you don't need to use the LLM Prompt Result token and Say node. By default, this result won't be stored in either the Input or the Context. You can change this behavior by activating the **Store Copy in Input** option. Note that streaming may not be supported by all [Cognigy LLM Prompt Node](../../../empower/llms/model-support-by-feature.md) providers, such as Google[^*]. If streaming is not supported, enabling the **Store Copy in Input** option will save the result to the Input object. |
| Input Key to store Result     | CognigyScript | The parameter appears when **Store in Input** is selected. The result is stored in the `promptResult` Input object by default. You can specify another value.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| Context Key to store Result   | CognigyScript | The parameter appears when **Store in Context** is selected. The result is stored in the `promptResult` Context object by default. You can specify another value.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
| Stream Buffer Flush Tokens    | Text Array    | The parameter appears when **Stream to Output** is selected. It defines tokens that trigger the stream buffer to flush to the output. The tokens can be punctuation marks or symbols, such as `\n`.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
| Stream Buffer Flush Overrides | Text Array    | The parameter appears when **Stream to Output** is selected. It allows using regular expressions (without leading or trailing slashes) to control stream buffer flushing. A trailing `$` is automatically added to match patterns at the end of the buffer. For example, `\d+\.` checks for a number followed by a dot at the end of the string.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
| Output result immediately     | Toggle        | The parameter appears when you select either **Store in Input** or **Store in Context**. This parameter allows you to output results immediately without using the Say Node and LLM Prompt token.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
| Store Detailed Results        | Toggle        | The parameter appears when you select either **Store in Input** or **Store in Context**, or when you enable **Store Copy in Input**. This parameter allows you to save detailed results of the LLM's generated output. By default, the result is stored in the `promptResult` object. You can specify another value in the **Context Key for the Result** field to save it in the Context object, or in the **Input Key for the Result** to save it in the Input object. The object contains keys such as `result`, `finishReason`, and `usage`. It may also include `detailedResult` if completion models are used, as well as `firstChunk` and `lastChunk` in some streaming results, depending on the LLM provider.                                                                                                                                                                                                                                                                                                                                                                                                        | 
| Store Copy in Input           | Toggle        | The parameter appears when **Stream to Output** is selected. In addition to streaming the result to the output, store a copy in the Input object by specifying a value in the **Input Key to store Result** field.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
| Input Key to store Result     | CognigyScript | The parameter appears when **Store Copy in Input** is selected. The result is stored in the `promptResult` Input object by default. You can specify another value.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |

### Advanced

| Parameter                 | Type     | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
|---------------------------|----------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| LLM                       | Selector | Determine how to handle the prompt result:<br> - **Store in Input** — stores the result in the Input object. To print the prompt result, use the LLM Prompt Result Token in the Say Node or enable the **Output result immediately** option. <br> - **Store in Context** — stores the result in the Context object. To print the prompt result, use the LLM Prompt Result Token in the Say Node or enable the **Output result immediately** option. <br>- **Stream to Output** — streams the result directly into the output. This means that the model provides prompts directly into the conversation chat, and you don't need to use the LLM Prompt Result token and Say node. By default, this result won't be stored in either the Input or the Context. You can change this behavior by activating the **Store Copy in Input** option. Note that streaming may not be supported by all [Cognigy LLM Prompt Node](../../../empower/llms/model-support-by-feature.md) providers, such as Google[^*]. If streaming is not supported, enabling the **Store Copy in Input** option will save the result to the Input object. |
| AI Agent Base Version     | Selector | Select one of the Agentic AI versions:<br>`1` - stable version but could lack some advanced features, optimizations, or bug fixes found in later versions.<br>`Latest` -  the most recent update of the Agentic AI feature, which may include new features, bug fixes, security patches, performance improvements, and other updates that were made since Version 1.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
| Tool Choice               | Selector | Select the option for how tools should be handled in the Agent AI job:<br>- Auto — tools are used automatically by the system when needed.<br>- Required — the tool is used only when the user requests it via user input.<br>- None — no tool is needed for the task; the system operates without it.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| Timeout                   | Number   | Define the maximum number of milliseconds to wait for a response from the Generative AI Provider.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
| Maximum Completion Tokens | Slider   | Define the maximum number of tokens to control how long or short the AI's response can be. For example, if you set the maximum completion tokens to 100, the AI will stop generating content after it has used up 100 tokens (which could be roughly 100 words, depending on the language and tokenization method).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |

### Error Handling

| Parameter                      | Type          | Description                                                                                                                                                                                                                                                                                                                                                                                            |
|--------------------------------|---------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Log to System Logs             | Toggle        | Log errors to the system logs. They can be viewed on the [Logs](../../../test/logs.md) page of your Project. The parameter is inactive by default.                                                                                                                                                                                                                                                     |
| Store in Input                 | Toggle        | Store errors in the Input object.                                                                                                                                                                                                                                                                                                                                                                      |
| Select Error Handling Approach | Select        | You can select one of the Error Handling options:<br>- **Stop Flow Execution** — terminate the current Flow execution.<br>- **Continue Flow Execution** — allow the Flow to continue executing, bypassing the error and proceeding to the next steps.<br>- **Go to Node** — redirect the workflow to a specific Node in the Flow, which can be useful for error recovery or customized error handling. |
| Error Message (optional)       | Text          | The parameter appears when **Continue Flow Execution** is selected. Add an message to output if the LLM Prompt Node fails.                                                                                                                                                                                                                                                                             |
| Select Flow                    | Select        | The parameter appears when **Go to Node** is selected. Select a Flow from the available options.                                                                                                                                                                                                                                                                                                       |
| Select Node                    | Select        | The parameter appears when **Go to Node** is selected. Select a Node from the available options.                                                                                                                                                                                                                                                                                                       |
| Error Message (optional)       | CognigyScript | Optional message to output if the LLM Prompt service fails                                                                                                                                                                                                                                                                                                                                             |

### Debug Settings

| Parameter       | Type   | Description                                                       |
|-----------------|--------|-------------------------------------------------------------------|
| Log Config      | Toggle | Sends a Debug Message with the current AI Agent Job configuration |
| Log Result      | Toggle | Sends the result from the LLM as a Debug Message.                 |
| Log Token Count | Toggle | Sends token count for request and completion as a Debug Message.  |

## AI Agent Tool Settings

This configuration defines a tool, sets its parameters, and allows for debugging by enabling detailed messages about the tool's execution.

### Tool

| Parameter   | Type          | Description                                                                                                                                          |
|-------------|---------------|------------------------------------------------------------------------------------------------------------------------------------------------------|
| Tool ID     | CognigyScript | Provide a meaningful name as a Tool ID. This ID can contain only letters, numbers, underscores (`_`), or dashes (`-`). For example, `update_user-1`. |
| Description | CognigyScript | A detailed description of what the tool does, when it should be used, and how it behaves.                                                            |

### Parameters

Configure the parameters that will be collected by the AI Agent before the tool is called. You can switch between the Graphical and JSON editors. When editing the JSON, follow the [JSON Schema specification](https://json-schema.org).

| Parameter       | Type          | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
|-----------------|---------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Use Parameters  | Toggle        | Activate this toggle if you want to add parameters in addition to you tool name and description. A JSON Schema object defining whether the expected parameters should be used or not for the tool.                                                                                                                                                                                                                                                                                                                                                                                                                  |
| Name            | Text          | Specify the name of the parameter. The hane should be clear and concise and describe the purpose of parameter.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
| Type            | Selector      | Select a type of the parameter: <br>**String** — a sequence of characters. For example, `"hello"`, `"123"`.<br>**Number** — a numerical value, which can be either an integer (for example,`5`) or a floating point number (for example, `3.14`).<br>**Boolean** — a logical value representing `true` or `false`.<br>**Array** — a collection of elements, which can contain multiple values of any type. For example, `["apple", "banana", "cherry"]`).<br>**Object** — a collection of key-value pairs, where each key is a string and the value can be of any type. For example, `{"name": "John", "age": 30}`. |
| Description     | CognigyScript | Explain what parameter means by providing a brief description of the parameter's usage.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
| Enum (optional) | Enum          | Define a set of values that the parameter can accept. The enum restricts the input to one of the specified values, ensuring only valid options are chosen.                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
| Add Parameter   | Button        | Add a new parameter.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |

### Debug Message

| Parameter                 | Type   | Description                                                                                         |
|---------------------------|--------|-----------------------------------------------------------------------------------------------------|
| Debug Message when called | Toggle | Enable the output of a debug message when the tool is called, detailing information about the call. |

## More Information

- [Agentic AI](../../../empower/agentic-ai/overview.md)