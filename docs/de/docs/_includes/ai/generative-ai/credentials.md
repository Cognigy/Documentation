1. Öffnen Sie die Cognigy.AI-Schnittstelle.
2. Gehen Sie zu **Build > LLM**.
3. Klicken Sie auf **+Neuer LLM**.
4. Wählen Sie im Fenster **Neuer LLM** ein Modell aus der Liste **Modelltyp** aus.
5. Fügen Sie einen eindeutigen Namen und eine Beschreibung für Ihr Modell hinzu und klicken Sie auf **Speichern**.
6. Wechseln Sie im Fenster **LLM-Editor** zum Feld **Generative AI Connection**.
7. Klicken Sie auf der rechten Seite des Feldes auf **+**.
8. Gehen Sie je nach Modellanbieter wie folgt vor:

=== "Microsoft Azure OpenAI"
        8.1 Füllen Sie die folgenden Felder aus:<br>- **Verbindungsname** – Erstellen Sie einen eindeutigen Namen für Ihre Verbindung.<br>- **apiKey** – Fügen Sie einen [Azure-API-Schlüssel](https://learn.microsoft.com/en-us/azure/cognitive-services/openai/quickstart?tabs=command-line&pivots=rest-api#retrieve-key-and-endpoint) hinzu. Dieser Wert finden Sie im Abschnitt Schlüssel und Endpunkt, wenn Sie Ihre Ressource über das Azure-Portal untersuchen. Sie können entweder 'KEY1' oder 'KEY2' verwenden.<br>- **Ressourcenname** — Fügen Sie einen [Ressourcennamen](https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource) hinzu. Diesen Wert finden Sie im Azure-Portal unter **Ressourcenverwaltung > Bereitstellungen** oder alternativ unter **Verwaltung > Bereitstellungen** in Azure OpenAI Studio.<br>8.2 Klicken Sie auf **Erstellen**.<br>8.3 Füllen Sie die restlichen Felder aus:<br>- **Bereitstellungsname** – fügen Sie einen [Modellnamen](https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#deploy-a-model) hinzu.<br>- **API-Version** — Fügen Sie eine [API-Version](https://learn.microsoft.com/en-us/azure/cognitive-services/openai/reference#rest-api-versioning) hinzu. Die API-Version, die für diesen Vorgang im Format "JJJJ-MM-TT" verwendet werden soll. Beachten Sie, dass die Version ein erweitertes Format haben kann, z. B. "2023-03-15-preview". <br>- **Benutzerdefinierte URL** – dieser Parameter ist optional. Um die Verbindung zwischen Ihren Clustern und dem Azure OpenAI-Anbieter zu steuern, können Sie Verbindungen über dedizierte Proxyserver weiterleiten und so eine zusätzliche Sicherheitsebene schaffen. Geben Sie dazu die URL nach folgendem Muster an: '<resource-name>https://.openai.azure.com/openai/deployments/<deployment-name>/completions?api-version=<api-verson>'. Wenn eine benutzerdefinierte URL hinzugefügt wird, werden die Felder **Ressourcenname**, **Bereitstellungsname** und **API-Version** ignoriert.

=== "OpenAI"
        8.1 Füllen Sie die folgenden Felder aus:<br>- **Verbindungsname** – Erstellen Sie einen eindeutigen Namen für Ihre Verbindung.<br>- **apiKey** — fügen Sie einen API-Schlüssel aus Ihrem OpenAI-Konto hinzu. Diesen Schlüssel finden Sie in den [Benutzereinstellungen](https://help.openai.com/en/articles/4936850-where-do-i-find-my-secret-api-key) Ihres OpenAI-Kontos.<br>8.2 Klicken Sie auf **Erstellen**.<br>8.3 Füllen Sie das restliche Feld aus:<br>- **Benutzerdefiniertes Modell** - Geben Sie das jeweilige Modell an, das Sie verwenden möchten. Dieser Parameter ist hilfreich, wenn Sie mehrere Modelltypen auf der Seite des LLM-Anbieters haben und beabsichtigen, einen bestimmten Modelltyp zu verwenden. Wenn Sie beispielsweise GPT-4 haben, können Sie "gpt-4-0613" für Ihren Anwendungsfall angeben. Dieser Parameter ist optional. Wenn ein benutzerdefiniertes Modell hinzugefügt wird, wird das standardmäßige LLM-Modell ignoriert. Weitere Informationen zu den Modellen des Anbieters finden Sie in der [OpenAI-Dokumentation](https://platform.openai.com/docs/models/overview).<br>=== "Anthropisch"
        8.1 Füllen Sie die folgenden Felder aus:<br>- **Verbindungsname** – Erstellen Sie einen eindeutigen Namen für Ihre Verbindung.<br>- **apiKey** — fügen Sie einen API-Schlüssel hinzu, den Sie über [Kontoeinstellungen](https://console.anthropic.com/docs/api#accessing-the-api) in Anthropic generiert haben.<br>8.2 Klicken Sie auf **Erstellen**.<br>=== "Google"
        8.1 Füllen Sie das Feld **Verbindungsname** aus, indem Sie einen eindeutigen Namen für Ihre Verbindung angeben.<br>8.2 Um die JSON-Datei mit einem Schlüssel für Ihr Modell hochzuladen, müssen Sie diesen Schlüssel abrufen. Rufen Sie die Google Vertex AI-Konsole auf.<br>8.3 Klicken Sie auf die Schaltfläche **Alle empfohlenen APIs aktivieren**, um eine API-Verbindung zu aktivieren, falls diese nicht aktiviert ist. Stellen Sie sicher, dass die Vertex AI-API aktiviert ist.<br>8.4 Gehen Sie im Menü auf der linken Seite zu **IAM & Admin > Service Accounts**.<br>8.5 Wählen Sie **Aktionen** und klicken Sie auf **Schlüssel verwalten**.<br>8.6 Wählen Sie auf der Seite **Schlüssel** die Option **Schlüssel hinzufügen** aus, und klicken Sie auf **Neuen Schlüssel erstellen**.<br>8.7 Wählen Sie im angezeigten Fenster den Schlüsseltyp **JSON** aus und klicken Sie auf **Erstellen**. Die Datei wird heruntergeladen.<br>8.8 Klicken Sie in Cognigy im Fenster **Neue Verbindung** auf **JSON-Datei hochladen** und laden Sie die Datei hoch.<br>8.9 Klicken Sie auf **Erstellen**.<br>8.10 Füllen Sie die restlichen Felder aus:<br>- **Position** – Fügen Sie eine [Region](https://cloud.google.com/vertex-ai/docs/general/locations) für das Modell hinzu. Beispiel: 'us-central1'.<br>- **API-Endpunkt** – fügen Sie einen [Dienstendpunkt](https://cloud.google.com/vertex-ai/docs/reference/rest#service-endpoint) für das Modell hinzu. Beispiel: "us-central1-aiplatform.googleapis.com". Beachten Sie, dass der Endpunkt ohne "https://" oder "http://" angegeben werden sollte. <br>- **Publisher** – Fügen Sie den Namen eines Besitzers des Modells hinzu. Wenn nicht angegeben, wird standardmäßig "Google" verwendet.
               Dieser Parameter ist optional.<br>=== "Aleph Alpha"
        8.1 Füllen Sie die folgenden Felder aus:<br>- **Verbindungsname** – Erstellen Sie einen eindeutigen Namen für Ihre Verbindung.<br>- **Token*** — Geben Sie einen Schlüssel an, den Sie in Ihrem [Aleph Alpha-Konto](https://docs.aleph-alpha.com/docs/account/#create-a-new-token) erstellt haben.<br>8.2 Klicken Sie auf **Erstellen**.<br>8.3 Füllen Sie das restliche Feld aus:<br>- **Benutzerdefiniertes Modell** - Geben Sie ein bestimmtes Modell an, das Sie verwenden möchten, z. B. 'luminous-base'. Dieser Parameter ist optional. Wenn ein benutzerdefiniertes Modell hinzugefügt wird, wird das standardmäßige LLM-Modell ignoriert. Weitere Informationen zu den Modellen des Anbieters finden Sie in der [Aleph Alpha-Dokumentation](https://docs.aleph-alpha.com/docs/introduction/model-card).<br>  9. Um die Änderungen zu übernehmen, klicken Sie auf **Speichern**.
10. Um zu überprüfen, ob die Verbindung eingerichtet wurde, klicken Sie auf **Test**.

Wenn das Modell hinzugefügt wird, wird es in der Liste der Modelle angezeigt.

Um dieses Modell für Cognigy-Features anzuwenden, wechseln Sie zu den Einstellungen, indem Sie auf **LLM-Features verwalten** klicken.</api-verson></deployment-name></resource-name>